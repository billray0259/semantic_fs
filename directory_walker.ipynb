{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "import base64\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # read every page and concatenate them\n",
    "    pdf = PdfReader(file_path)\n",
    "    return \"\\n\".join([page.extract_text() for page in pdf.pages])\\\n",
    "\n",
    "\n",
    "supported_file_types = {\n",
    "    '.txt': read_txt,\n",
    "    '.pdf': read_pdf\n",
    "}\n",
    "\n",
    "def iter_supported_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] in supported_file_types:\n",
    "                yield os.path.join(root, file)\n",
    "\n",
    "\n",
    "def iter_texts(directory):\n",
    "    for file in iter_supported_files(directory):\n",
    "        try:\n",
    "            yield (file, supported_file_types[os.path.splitext(file)[1]](file))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "class TextEncoderPipeline:\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\", chunk_size=128):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = self.model.tokenizer.tokenize(text)\n",
    "        chunks = [tokens[i:i+self.chunk_size] for i in range(0, len(tokens), self.chunk_size)]\n",
    "        chunks.extend(\n",
    "            [tokens[i:i+self.chunk_size] for i in range(self.chunk_size//2, len(tokens), self.chunk_size)]\n",
    "        )\n",
    "        # convert the chunks of tokens back to text\\\n",
    "        text_chunks = [self.model.tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]\n",
    "        embeddings = self.model.encode(text_chunks)\n",
    "\n",
    "        # return both the embeddings and the text chunks\n",
    "        return text_chunks, embeddings\n",
    "    \n",
    "\n",
    "class QueryEncoderPipeline:\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, query):\n",
    "        return self.model.encode(query)\n",
    "\n",
    "\n",
    "def ndarray_to_json(ndarray):\n",
    "    return {\n",
    "        \"shape\": ndarray.shape,\n",
    "        \"type\": str(ndarray.dtype),\n",
    "        \"bytes\": base64.b64encode(ndarray.tobytes()).decode(\"utf-8\")\n",
    "    }\n",
    "\n",
    "\n",
    "def ndarray_from_json(json):\n",
    "    return np.frombuffer(base64.b64decode(json[\"bytes\"]), dtype=json[\"type\"]).reshape(json[\"shape\"])\n",
    "\n",
    "encoder_pipeline = TextEncoderPipeline(chunk_size=128)\n",
    "\n",
    "def get_embeddings_dict(data_dir):\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    for file, text in iter_texts(data_dir):\n",
    "        text_chunks, embeddings = encoder_pipeline(text)\n",
    "        for chunk, embedding in zip(text_chunks, embeddings):\n",
    "            # hash the text to make a key\n",
    "            key = str(uuid.uuid3(uuid.NAMESPACE_DNS, chunk))\n",
    "            embeddings_dict[key] = {}\n",
    "            embeddings_dict[key][\"filename\"] = file\n",
    "            embeddings_dict[key][\"text\"] = chunk\n",
    "            embeddings_dict[key][\"embedding\"] = ndarray_to_json(embedding)\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "data_dir = \"data_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (66268 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file data_directory/pdfs/02_CS687.pdf: EOF marker not found\n",
      "Error reading file data_directory/pdfs/01_CS687.pdf: EOF marker not found\n",
      "Error reading file data_directory/pdfs/03_CS687.pdf: EOF marker not found\n",
      "Error reading file data_directory/pdfs/04_CS687.pdf: EOF marker not found\n"
     ]
    }
   ],
   "source": [
    "with open(\"embeddings.json\", 'w') as f:\n",
    "    json.dump(get_embeddings_dict(data_dir), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QueryEncoderPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bill/fall2022/596/semantic_fs/directory_walker.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/fall2022/596/semantic_fs/directory_walker.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m query_encoder_pipeline \u001b[39m=\u001b[39m QueryEncoderPipeline()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/fall2022/596/semantic_fs/directory_walker.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery_embeddings_dict\u001b[39m(query, embeddings_dict, top_k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/fall2022/596/semantic_fs/directory_walker.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     query_embedding \u001b[39m=\u001b[39m query_encoder_pipeline(query)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QueryEncoderPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "query_encoder_pipeline = QueryEncoderPipeline()\n",
    "\n",
    "def query_embeddings_dict(query, embeddings_dict, top_k=10):\n",
    "    query_embedding = query_encoder_pipeline(query)\n",
    "    results = []\n",
    "    for key, value in embeddings_dict.items():\n",
    "        embedding = ndarray_from_json(value[\"embedding\"])\n",
    "        results.append((key, value[\"filename\"], value[\"text\"], np.dot(query_embedding, embedding)))\n",
    "    return sorted(results, key=lambda x: x[3], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", tokenizer=\"deepset/roberta-base-squad2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 27.476634979248047\n",
      "\tAnswer: a tuple\n",
      "\tContext: state of the environment is st, the agent takes action at, and the environment transitions to state st + 1, the agent receives the reward rt. this differs from some other sources wherein this reward is called rt + 1. there are many definitions of mdps used in the literature, which share common terms. in each case an mdp is a tuple. four examples are : 1. ( s, a, p, r ) 2. ( s, a, p, r, γ ) 3. ( s, a, p, r, d 0, γ ) 4. ( s, a, p, d r\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 27.319942474365234\n",
      "\tAnswer: four common ways of defining an mdp. these different definitions\n",
      "\tContext: , a, r, s ′, a ′ ) later. • d0is the initial state distribution : d0 : s → [ 0, 1 ], ( 5 ) and for all s : d0 ( s ) = pr ( s0 = s ). ( 6 ) 1in the remainder of the course, we will very rarely use dr — typically we will work with r. 8 • γ∈ [ 0, 1 ] is a parameter called the reward discount parameter, and which we discuss later. recall now our earlier list of four common ways of defining an mdp. these different definitions vary in how precisely they define\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 27.098356246948242\n",
      "\tAnswer: a tuple\n",
      "\tContext: case an mdp is a tuple. four examples are : 1. ( s, a, p, r ) 2. ( s, a, p, r, γ ) 3. ( s, a, p, r, d 0, γ ) 4. ( s, a, p, d r, d0, γ ). 7 we will discuss the differences between these definitions in a moment, but first let ’ s define each of the terms. notice that the unique terms in these definitions are : s, a, p, d r, r, d 0, andγ. we define each of these below\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 26.355714797973633\n",
      "\tAnswer: s × a → r\n",
      "\tContext: some constant rmax∈r. 1 • ris a function called the reward function, which is implicitly defined by dr. other sources often define an mdp to contain rrather than dr. formally r : s × a → r, ( 3 ) and r ( s, a ) : = e [ rt | st = s, at = a ], ( 4 ) for all s, a, andt. although the reward function, r, does not precisely define how the rewards, rt, are generated ( and thus a definition of an mdp with rin place of drwould in a way be incomplete\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 26.197223663330078\n",
      "\tAnswer: mdps\n",
      "\tContext: ##≥0be the time step ( iteration of the agent - environment loop ). • letstbe the state of the environment at time t. • letatbe the action taken by the agent at time t. • letrt∈rbe the reward received by the agent at time t. that is, when the state of the environment is st, the agent takes action at, and the environment transitions to state st + 1, the agent receives the reward rt. this differs from some other sources wherein this reward is called rt + 1. there are many definitions of mdps used in the literature, which share common terms. in each\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 25.995452880859375\n",
      "\tAnswer: the environment\n",
      "\tContext: course, we will very rarely use dr — typically we will work with r. 8 • γ∈ [ 0, 1 ] is a parameter called the reward discount parameter, and which we discuss later. recall now our earlier list of four common ways of defining an mdp. these different definitions vary in how precisely they define the environment. the definition ( s, a, p, r, γ ) contains all of the terms necessary for us to reason about optimal behavior of an agent. the definition ( s, a, p, r ) still actually includes γ, it just makes it implicit. that is, this definition assumes that γis\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 25.83753204345703\n",
      "\tAnswer: mathematically\n",
      "\tContext: still present, but doesn ’ t write it as one of the terms in the mdp definition. on the other extreme, the definition ( s, a, p, d r, d0, γ ) fully specifies how the environment behaves. this distinction is most clear when considering the inclusion of drrather than r. as we will see later, the expected rewards described by rare all that is needed to reason about what behavior is optimal. however, to fully characterize how rewards are generated in an environment, we must specify dr. just as we have defined the environment mathematically, we now define the agent mathematically. a\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 25.167510986328125\n",
      "\tAnswer: implicit\n",
      "\tContext: the environment. the definition ( s, a, p, r, γ ) contains all of the terms necessary for us to reason about optimal behavior of an agent. the definition ( s, a, p, r ) still actually includes γ, it just makes it implicit. that is, this definition assumes that γis still present, but doesn ’ t write it as one of the terms in the mdp definition. on the other extreme, the definition ( s, a, p, d r, d0, γ ) fully specifies how the environment behaves. this distinction is most clear when considering the inclusion of drrather than\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 24.350784301757812\n",
      "\tAnswer: optimal policy\n",
      "\tContext: mdps where | s | < ∞, | a | < ∞, rmax < ∞, andγ < 1, there exists at least one optimal policy, π∗under this definition of an optimal policy. that is, property 1 holds for this definition of an optimal policy as well as the definition of an optimal policy in ( 17 ). from the definition of v∗in ( 114 ), it follows that v∗ = vπ∗for all optimal policies, π∗. we now have two different definitions of an optimal policy. both definitions are standard in rl research. the definition presented in ( 17 ) is common in\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 24.18252182006836\n",
      "\tAnswer: so that the markov property is satisfied\n",
      "\tContext: 18 to enforce the markov property. this is typically undesirable because the size of the state set grows exponentially with the maximum episode length ( a term discussed more later ). this trick of adding information into the state is called state augmentation. there is often confusion about terminology surrounding states, state repre - sentations, and the markov property. the state of an mdp ( and every other similar formulation, like pomdps, dec - pomdps, smdps, etc. ) should always be defined so that the markov property is satisfied. later we will reason about state representations that are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"embeddings.json\", 'r') as f:\n",
    "    embeddings_dict = json.load(f)\n",
    "\n",
    "query = \"How do you formally define an MDP?\"\n",
    "results = query_embeddings_dict(query, embeddings_dict)\n",
    "\n",
    "for result in results:\n",
    "\n",
    "    context = result[2]\n",
    "    answer = qa_model(question = query, context = context)\n",
    "\n",
    "    print(f\"{result[1]} - {result[3]}\")\n",
    "    print(f\"\\tAnswer: {answer['answer']}\")\n",
    "    print(f\"\\tContext: {context}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd949634739cb2e3553531dc888c165434c8f42131e286f52dc6a7c1de32f5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
