{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "import base64\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEncoderPipeline:\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, query):\n",
    "        return self.model.encode(query)\n",
    "\n",
    "query_encoder_pipeline = QueryEncoderPipeline()\n",
    "\n",
    "def ndarray_from_json(json):\n",
    "    return np.frombuffer(base64.b64decode(json[\"bytes\"]), dtype=json[\"type\"]).reshape(json[\"shape\"])\n",
    "\n",
    "def query_embeddings_dict(query, embeddings_dict, top_k=10):\n",
    "    query_embedding = query_encoder_pipeline(query)\n",
    "    results = []\n",
    "    for key, value in embeddings_dict.items():\n",
    "        embedding = ndarray_from_json(value[\"embedding\"])\n",
    "        results.append((key, value[\"filename\"], value[\"text\"], np.dot(query_embedding, embedding)))\n",
    "    return sorted(results, key=lambda x: x[3], reverse=True)[:top_k]\n",
    "\n",
    "\n",
    "\n",
    "qa_model = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", tokenizer=\"deepset/roberta-base-squad2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 30.684228897094727\n",
      "\tAnswer: indefinite horizon\n",
      "\tContext: cause a transition tos∞after twenty seconds, the state must encode the current time step. 1. 7. 5 finite - horizon mdps thehorizon, l, of an mdp is the smallest integer such that [UNK], pr ( st = s∞ ) = 1. ( 26 ) ifl < ∞for all policies, then we say that the mdp is finite horizon. ifl = ∞ then the domain may be indefinite horizon orinfinite horizon. an mdp with indefinite horizon is one for which l = ∞, but where the agent will always enter s∞. one example of an indefinite\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 30.071758270263672\n",
      "\tAnswer: an mdp where the agent may never enter s∞.\n",
      "\tContext: horizon mdp is one where the agent transitions to s∞with probability 0. 5 from every state. an infinite horizon mdp is an mdp where the agent may never enter s∞. for the cart - pole domain, how can we implement within pthat a transition tos∞must occur after 20 seconds? we achieve this by augmenting the state to include the current time. that is, the state is ( s, t ), where sis what we previously defined to be the state for cart - pole and tis the current time step. the transition function, p, increments tat each time step and\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 29.056087493896484\n",
      "\tAnswer: an mdp where the agent may never enter s∞.\n",
      "\tContext: ##l < ∞for all policies, then we say that the mdp is finite horizon. ifl = ∞ then the domain may be indefinite horizon orinfinite horizon. an mdp with indefinite horizon is one for which l = ∞, but where the agent will always enter s∞. one example of an indefinite horizon mdp is one where the agent transitions to s∞with probability 0. 5 from every state. an infinite horizon mdp is an mdp where the agent may never enter s∞. for the cart - pole domain, how can we implement within pthat a transition tos∞must occur after 20 seconds\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 24.74523162841797\n",
      "\tAnswer: l = 10\n",
      "\tContext: define gλ tto be the limit as λ→1, i. e., the monte carlo return. to better understand what the λ - return is doing, consider the weights that would be placed on the different length returns for an mdp with finite horizon, l = 10. the weight placed on the 1 - step return would be ( 1 −λ ), the weight on the 2 - step return would be ( 1 −λ ) λ, the weight on the 3 - step return would be ( 1−λ ) λ2,..., the weight on the 10 - step return would be ( 1 −λ\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 22.593828201293945\n",
      "\tAnswer: tuple\n",
      "\tContext: state of the environment is st, the agent takes action at, and the environment transitions to state st + 1, the agent receives the reward rt. this differs from some other sources wherein this reward is called rt + 1. there are many definitions of mdps used in the literature, which share common terms. in each case an mdp is a tuple. four examples are : 1. ( s, a, p, r ) 2. ( s, a, p, r, γ ) 3. ( s, a, p, r, d 0, γ ) 4. ( s, a, p, d r\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 22.562223434448242\n",
      "\tAnswer: bayesian network\n",
      "\tContext: , · ) ; 5 [UNK] ( st, at, st + 1 ) ; the running of an mdp is also presented as a bayesian network in figure 4. [UNK] [UNK] [UNK] [UNK] + 1 [UNK] [UNK] + [UNK] + 1 figure 4 : bayesian network depicted the running of an mdp. notice that we have defined rewards so that r0is the first reward, while sutton and barto ( 1998 ) define rewards such that r1is the first reward. we do this because s0, a0, and t = 0 are the first state, action, and time, and so having r1be the first reward would\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 22.536447525024414\n",
      "\tAnswer: environment loop\n",
      "\tContext: ##≥0be the time step ( iteration of the agent - environment loop ). • letstbe the state of the environment at time t. • letatbe the action taken by the agent at time t. • letrt∈rbe the reward received by the agent at time t. that is, when the state of the environment is st, the agent takes action at, and the environment transitions to state st + 1, the agent receives the reward rt. this differs from some other sources wherein this reward is called rt + 1. there are many definitions of mdps used in the literature, which share common terms. in each\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 22.518657684326172\n",
      "\tAnswer: l = 10\n",
      "\tContext: 297 ) notice that if λ = 0 the λ - return is the td return. in the limit as λ→1, the λ - return is the monte carlo return. when λ = 1, gλ tas defined above is not necessarily defined, since it could become infinity times zero. hence we explicitly re - define gλ tto be the limit as λ→1, i. e., the monte carlo return. to better understand what the λ - return is doing, consider the weights that would be placed on the different length returns for an mdp with finite horizon, l = 10. the weight placed on the 1\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 22.452810287475586\n",
      "\tAnswer: rrather\n",
      "\tContext: some constant rmax∈r. 1 • ris a function called the reward function, which is implicitly defined by dr. other sources often define an mdp to contain rrather than dr. formally r : s × a → r, ( 3 ) and r ( s, a ) : = e [ rt | st = s, at = a ], ( 4 ) for all s, a, andt. although the reward function, r, does not precisely define how the rewards, rt, are generated ( and thus a definition of an mdp with rin place of drwould in a way be incomplete\n",
      "\n",
      "data_directory/pdfs/Lecture_Notes_v1.0_687_F22.pdf - 22.348445892333984\n",
      "\tAnswer: finite states and actions\n",
      "\tContext: ##imum likelihood model — the estimates of panddrthat maximize the probability that we would see the data we have. the maximum likelihood model for an mdp with finite states and actions is exactly what one might expect. that is : [UNK] ( s, a, s ′ ) = # ( s, a, s ′ ) # ( s, a ) ( 245 ) [UNK] ( s, a ) = mean ( r | s, a ), ( 246 ) where # ( s, a, s ′ ) is the number of occurrences of ( s, a, s ′ ) in our data, # ( s, a ) is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"embeddings.json\", 'r') as f:\n",
    "    embeddings_dict = json.load(f)\n",
    "\n",
    "query = \"What is an infinite horizon MDP?\"\n",
    "results = query_embeddings_dict(query, embeddings_dict)\n",
    "\n",
    "for result in results:\n",
    "\n",
    "    context = result[2]\n",
    "    answer = qa_model(question = query, context = context)\n",
    "\n",
    "    print(f\"{result[1]} - {result[3]}\")\n",
    "    print(f\"\\tAnswer: {answer['answer']}\")\n",
    "    print(f\"\\tContext: {context}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd949634739cb2e3553531dc888c165434c8f42131e286f52dc6a7c1de32f5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
